{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import config\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from model_selection import model_Selection\n",
    "import train\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tester as test\n",
    "from torch_geometric.utils import degree\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return 100<=data.num_nodes <= 1000\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(acc_list,loss_list,typedef):\n",
    "    x = np.arange(len(acc_list))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    xnew = np.linspace(x.min(),x.max(),500)\n",
    " \n",
    "    acc_list_smooth = make_interp_spline(x, acc_list)(xnew)\n",
    "    \n",
    "    plt.plot(xnew, acc_list_smooth, '-')\n",
    "    plt.title('{} accuracy vs. epoches'.format(typedef))\n",
    "    plt.ylabel('{} accuracy'.format(typedef))\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    loss_list_smooth = make_interp_spline(x, loss_list)(xnew)\n",
    "    plt.plot(xnew, loss_list_smooth, '-')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('{} loss'.format(typedef))\n",
    "    plt.show()\n",
    "    plt.savefig(\"accuracy_loss.jpg\")\n",
    "\n",
    "\n",
    "class add_degree(object):\n",
    "    r\"\"\"Adds a constant value to each node feature.\n",
    "\n",
    "    Args:\n",
    "        value (int, optional): The value to add. (default: :obj:`1`)\n",
    "        cat (bool, optional): If set to :obj:`False`, all existing node\n",
    "            features will be replaced. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=1, cat=True):\n",
    "        self.value = value\n",
    "        self.cat = cat\n",
    "\n",
    "    def __call__(self, data):\n",
    "        x = data.x\n",
    "        \n",
    "        c = torch.full((data.num_nodes, 1), self.value)\n",
    "\n",
    "        if x is not None and self.cat:\n",
    "            x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "            data.x = torch.cat([x, c.to(x.dtype).to(x.device)], dim=-1)\n",
    "            data.x[:,-1]=torch.unsqueeze(degree(data.edge_index[0]),0)\n",
    "        else:\n",
    "            data.x = c\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(value={})'.format(self.__class__.__name__, self.value)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "                    \n",
    "#def hyperparameters_search(args):\n",
    "\n",
    "'''\n",
    "number_training = int(len(dataset)*0.2)*4\n",
    "number_val = int(len(dataset)*0.2)\n",
    "number_test = len(dataset) - number_training\n",
    "number_list=[]\n",
    "dataset_list=[]\n",
    "for i in range(5):\n",
    "    if i == 4:\n",
    "        number_list.append(number_test)\n",
    "    else:\n",
    "        number_list.append(number_val)\n",
    "print(number_list)\n",
    "dataset_list=list(random_split(dataset,number_list))\n",
    "\n",
    "for i in range(5):\n",
    "    best_parameters=[]\n",
    "    \n",
    "    #training_set,validation_set,test_set = random_split(dataset,[number_training,number_val,number_test])\n",
    "    print(i)\n",
    "    test_set=dataset_list[i]\n",
    " \n",
    "    temp=dataset_list.copy()\n",
    "    temp.pop(i)\n",
    "    training_set=ConcatDataset(temp)\n",
    "    number_traintemp = int(len(training_set)*0.8)\n",
    "    number_testtemp = len(training_set)-number_traintemp\n",
    "    training_set,validation_set=random_split(training_set,[number_traintemp,number_testtemp])\n",
    "\n",
    "    train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(validation_set,batch_size=args.batch_size,shuffle=False)\n",
    "    test_loader = DataLoader(test_set,batch_size=1,shuffle=False)\n",
    "    \n",
    "    #best_parameters.append(model_Selection(args,train_loader,val_loader,test_loader))\n",
    "    acctest_list,losstest_list,accval_list,lossval_list=train.train_process(args,train_loader,val_loader,test_loader)\n",
    "    #plot_result( acctest_list,losstest_list,\"test\")\n",
    "    plot_result(accval_list,lossval_list,\"validation\")\n",
    "    \n",
    "'''\n",
    "\"\"\"\n",
    "acc_list=[]\n",
    "loss_list=[]\n",
    "\n",
    "for i in [\"SAGPool\",\"gPool\",\"edgepool\",\"DiffPool\"]:\n",
    "    for j in range(4):\n",
    "        print(j)\n",
    "        args = config.get_config()\n",
    "        torch.manual_seed(args.seed)\n",
    "        dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,pre_filter=MyFilter(),use_node_attr=True)\n",
    "        args.num_classes = dataset.num_classes\n",
    "        args.num_features = dataset.num_features\n",
    "        num_training = int(len(dataset)*0.8)\n",
    "        num_val = int(len(dataset)*0.1)\n",
    "        num_test = len(dataset) - (num_training+num_val)\n",
    "        training_set,validation_set,test_set = random_split(dataset,[num_training,num_val,num_test])\n",
    "        train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(validation_set,batch_size=args.batch_size,shuffle=False)\n",
    "        test_loader = DataLoader(test_set,batch_size=1,shuffle=False)\n",
    "        args.pool_method=i\n",
    "        accval_list,lossval_list=train.train_process(args,train_loader,val_loader,test_loader)\n",
    "        acc_list.append(accval_list)\n",
    "        loss_list.append(lossval_list)\n",
    "print(acc_list)\n",
    "print(loss_list)\n",
    "#plot_result(acc_list,loss_list,\"validation\")    \n",
    "\n",
    "\"\"\"\n",
    "args = config.get_config()\n",
    "torch.manual_seed(args.seed)\n",
    "for data in [\"DD\",\"PROTEINS\",\"ENZYMES\"]:\n",
    "    args.dataset=data\n",
    "    for pooling_methods in [\"DiffPool\",\"SAGPool\",\"edgepool\"]:#'SAGPool/gPool/edgepool/DiffPool'\n",
    "        args.pool_method=pooling_methods\n",
    "        dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,pre_filter=MyFilter(),use_node_attr=True)\n",
    "        number_training = int(len(dataset)*0.25)*3\n",
    "        number_val = int(len(dataset)*0.25)\n",
    "        number_test = len(dataset) - number_training\n",
    "        number_list=[]\n",
    "        dataset_list=[]\n",
    "        for i in range(4):\n",
    "            if i == 3:\n",
    "                number_list.append(number_test)\n",
    "            else:\n",
    "                number_list.append(number_val)\n",
    "        \n",
    "        cross_loss=[]\n",
    "        cross_accuracy=[]\n",
    "        cross_test_accuracy=[]\n",
    "        for i in range(4):\n",
    "            \n",
    "            best_accuracy=0\n",
    "            best_add_node=None\n",
    "            best_batch_size=0\n",
    "            best_hidden_size=0\n",
    "            best_learning_rate=0\n",
    "            best_con_number=None\n",
    "            best_patience=0\n",
    "            for add_node in [True,False]:\n",
    "                args.add_node=add_node\n",
    "                if not args.add_node:\n",
    "                    dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,pre_filter=MyFilter(),use_node_attr=True)\n",
    "                else:\n",
    "                    dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,transform=add_degree(value=1),pre_filter=MyFilter(),use_node_attr=True)\n",
    "                args.num_classes = dataset.num_classes\n",
    "                args.num_features = dataset.num_features\n",
    "                dataset_list=list(random_split(dataset,number_list)) \n",
    "                test_set=dataset_list[i]\n",
    "                temp=dataset_list.copy()\n",
    "                temp.pop(i)\n",
    "                training_set=ConcatDataset(temp)\n",
    "                number_trainselect = int(len(training_set)*0.6)\n",
    "                number_testselect = len(training_set)-number_trainselect\n",
    "                training_select,validation_select=random_split(training_set,[number_trainselect,number_testselect])\n",
    "                #the above is preparing the datasetfor \n",
    "                for learning_rate in [0.01,0.001]:\n",
    "                #for learning_rate in [0.01]:\n",
    "                    args.lr=learning_rate\n",
    "                    #for batch_size in [64]:\n",
    "                    for batch_size in [64,32]: \n",
    "                        print(batch_size)\n",
    "                        args.batch_size=batch_size\n",
    "                        for hidden_size in [64,128]:\n",
    "                        #for hidden_size in [128]:\n",
    "                            args.hidden_size=hidden_size\n",
    "                            for patience in [15,30]:\n",
    "                            #for patience in [20]:\n",
    "                                args.patience=patience\n",
    "                                for twoconv in [False,True]:\n",
    "                                #for twoconv in [False]:\n",
    "                                    args.two_conv=twoconv\n",
    "                                    train_loader = DataLoader(training_select, batch_size=args.batch_size, shuffle=True)\n",
    "                                    val_loader = DataLoader(validation_select,batch_size=args.batch_size,shuffle=False)\n",
    "                                    accval_list,lossval_list=train.train_process(args,train_loader,val_loader)\n",
    "                                    if max(accval_list)>best_accuracy:\n",
    "                                        best_accuracy=max(accval_list)\n",
    "                                        best_add_node=args.add_node\n",
    "                                        best_batch_size=args.batch_size\n",
    "                                        best_hidden_size=args.hidden_size\n",
    "                                        best_learning_rate=args.lr\n",
    "                                        best_con_number=args.two_conv\n",
    "                                        best_patience=args.patience\n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "            args.add_node=best_add_node\n",
    "            args.batch_size=best_batch_size\n",
    "            args.hidden_size=best_hidden_size\n",
    "            args.lr=best_learning_rate\n",
    "            args.two_conv=best_con_number\n",
    "            args.patience=best_patience\n",
    "            '''\n",
    "            print('add_degree:{}'.format(args.add_node)+'\\n')\n",
    "            print('batch_size:{}'.format(args.batch_size)+'\\n')\n",
    "            print('hidden_size:{}'.format(args.hidden_size)+'\\n')\n",
    "            print('learning_rate:{}'.format( args.lr)+'\\n')\n",
    "            print('two_convolution_layer:{}'.format(args.two_conv)+'\\n')\n",
    "            print('patience:{}'.format(args.patience)+'\\n')\n",
    "            '''\n",
    "            if not args.add_node:\n",
    "                dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,pre_filter=MyFilter(),use_node_attr=True)\n",
    "            else:\n",
    "                dataset = TUDataset(os.path.join('data',args.dataset),name=args.dataset,transform=add_degree(value=1),pre_filter=MyFilter(),use_node_attr=True)  \n",
    "            args.num_classes = dataset.num_classes\n",
    "            args.num_features = dataset.num_features\n",
    "            dataset_list=list(random_split(dataset,number_list)) \n",
    "            test_set=dataset_list[i]\n",
    "            temp=dataset_list.copy()\n",
    "            temp.pop(i)\n",
    "            training_set=ConcatDataset(temp)\n",
    "            test_loader = DataLoader(test_set,batch_size=1,shuffle=False)\n",
    "            number_trainevalue = int(len(training_set)*0.8)\n",
    "            number_testevalue = len(training_set)-number_trainevalue\n",
    "            training_evalue,validation_evalue=random_split(training_set,[number_trainevalue,number_testevalue])\n",
    "            train_loader = DataLoader(training_evalue, batch_size=args.batch_size, shuffle=True)   \n",
    "            val_loader = DataLoader(validation_evalue,batch_size=args.batch_size,shuffle=False)        \n",
    "            accval_list,lossval_list=train.train_process(args,train_loader,val_loader,cross_number=i)\n",
    "            cross_loss.append(lossval_list)\n",
    "            cross_accuracy.append(accval_list)\n",
    "            model=torch.load('{}_model_{}_cross_vilidation{}.pkl'.format(args.dataset,args.pool_method,i))\n",
    "            cross_test_accuracy.append(test.testing(model,test_loader,args.device))\n",
    "            f=open('{}_model_{}_cross{}_best.txt'.format(args.dataset,args.pool_method,i),'w')\n",
    "            f.write('add_degree:{}'.format(args.add_node)+'\\n')\n",
    "            f.write('batch_size:{}'.format(args.batch_size)+'\\n')\n",
    "            f.write('hidden_size:{}'.format(args.hidden_size)+'\\n')\n",
    "            f.write('learning_rate:{}'.format( args.lr)+'\\n')\n",
    "            f.write('two_convolution_layer:{}'.format(args.two_conv)+'\\n')\n",
    "            f.write('patience:{}'.format(args.patience)+'\\n')\n",
    "            f.close()\n",
    "            \n",
    "        f=open('{}_model_{}_accuracy.txt'.format(args.dataset,args.pool_method),'w')\n",
    "        for accuracy in cross_test_accuracy:\n",
    "            f.write(str(accuracy)+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "        f=open('{}_model_{}_cross_accuracy.txt'.format(args.dataset,args.pool_method),'w')\n",
    "        for accuracy in cross_accuracy:\n",
    "            f.write(str(accuracy)+'\\n')\n",
    "        f.close()\n",
    "        f=open('{}_model_{}_cross_loss.txt'.format(args.dataset,args.pool_method),'w')\n",
    "        for loss in cross_loss:\n",
    "            f.write(str(loss)+'\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "x = np.arange(49)\n",
    "xnew = np.linspace(x.min(),x.max(),500)\n",
    "acc_list_smooth_SAG = make_interp_spline(x, acc_list[0][:49])(xnew)\n",
    "acc_list_smooth_g = make_interp_spline(x, acc_list[1][:49])(xnew)\n",
    "acc_list_smooth_edge = make_interp_spline(x, acc_list[2][:49])(xnew)\n",
    "acc_list_smooth_Diff = make_interp_spline(x, acc_list[3][:49])(xnew)\n",
    "acc_matrix=np.zeros((4,500))\n",
    "\n",
    "\"\"\"\n",
    "acc_matrix=np.concatenate((acc_list_smooth_SAG,acc_list_smooth_g),axis = 1)\n",
    "print(acc_matrix.size)\n",
    "acc_matrix=np.stack((acc_matrix,acc_list_smooth_edge),axis = 0)\n",
    "print(acc_matrix.size)\n",
    "acc_matrix=np.stack((acc_matrix,acc_list_smooth_Diff),axis = 0)\n",
    "\"\"\"\n",
    "acc_matrix[0,:]=acc_list_smooth_SAG \n",
    "acc_matrix[1,:]=acc_list_smooth_g\n",
    "acc_matrix[2,:]=acc_list_smooth_edge\n",
    "acc_matrix[3,:]=acc_list_smooth_Diff\n",
    "acc_list_smooth_mean = np.mean(acc_matrix,axis=0)\n",
    "acc_list_smooth_std =np.std(acc_matrix, axis=0)\n",
    "\n",
    "# Plot accurancy bands for training and test sets\n",
    "plt.fill_between(xnew, acc_list_smooth_mean - acc_list_smooth_std, acc_list_smooth_mean + acc_list_smooth_std, color=\"grey\",alpha=0.5)\n",
    "#plt.fill_between(xnew, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "plt.plot(xnew, acc_list_smooth_SAG, label=\"SAGpool\", color=\"Green\")\n",
    "plt.plot(xnew, acc_list_smooth_mean, label=\"Mean\", color=\"black\")\n",
    "plt.plot(xnew, acc_list_smooth_edge, label=\"Edgepool\", color=\"purple\")\n",
    "plt.plot(xnew, acc_list_smooth_Diff, label=\"Diffpool\", color=\"blue\")\n",
    "plt.plot(xnew, acc_list_smooth_g, label=\"gPool\", color=\"red\")\n",
    "# Create plot\n",
    "plt.title(\"Validation Curve\")\n",
    "def to_percent(temp, position):\n",
    "    return '%1.0f'%(100*temp) + '%'\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.xlabel(\"Epchos\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n",
    "\"\"\"\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "x = np.arange(33)\n",
    "xnew = np.linspace(x.min(),x.max(),500)\n",
    "#acc_list_smooth_SAG = make_interp_spline(x, acc_list[0][:33])(xnew)\n",
    "\n",
    "acc_matrix=np.zeros((16,500))\n",
    "for i,element in enumerate(acc_list):\n",
    "    acc_matrix[i,:]=make_interp_spline(x, element[:33])(xnew)\n",
    "#print(acc_matrix)\n",
    "acc_matrix_sag=acc_matrix[:4,:]\n",
    "acc_matrix_g=acc_matrix[:8,:]\n",
    "acc_matrix_edge=acc_matrix[:12,:]\n",
    "acc_matrix_diff=acc_matrix[:16,:]\n",
    "\n",
    "\n",
    "acc_list_sag_mean = np.mean(acc_matrix_sag,axis=0)\n",
    "acc_list_g_mean = np.mean(acc_matrix_g,axis=0)\n",
    "acc_list_edge_mean = np.mean(acc_matrix_edge,axis=0)\n",
    "acc_list_diff_mean = np.mean(acc_matrix_diff,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "acc_list_sag_std = np.std(acc_matrix_sag,axis=0)\n",
    "acc_list_g_std = np.std(acc_matrix_g,axis=0)\n",
    "acc_list_edge_std = np.std(acc_matrix_edge,axis=0)\n",
    "acc_list_diff_std = np.std(acc_matrix_diff,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(xnew, acc_list_sag_mean - acc_list_sag_std, acc_list_sag_mean + acc_list_sag_std, color=\"blue\",alpha=0.5)\n",
    "plt.plot(xnew, acc_list_sag_mean, label=\"SAGpool\", color=\"blue\")\n",
    "\n",
    "plt.fill_between(xnew, acc_list_g_mean - acc_list_g_std, acc_list_g_mean + acc_list_g_std, color=\"red\",alpha=0.5)\n",
    "plt.plot(xnew, acc_list_g_mean, label=\"Gpool\", color=\"red\")\n",
    "\n",
    "plt.fill_between(xnew, acc_list_edge_mean - acc_list_edge_std, acc_list_edge_mean + acc_list_edge_std, color=\"green\",alpha=0.5)\n",
    "plt.plot(xnew, acc_list_edge_mean, label=\"Edgepool\", color=\"green\")\n",
    "\n",
    "\n",
    "plt.fill_between(xnew, acc_list_diff_mean - acc_list_diff_std, acc_list_diff_mean + acc_list_diff_std, color=\"purple\",alpha=0.5)\n",
    "plt.plot(xnew, acc_list_diff_mean, label=\"Diffpool\", color=\"purple\")\n",
    "\n",
    "plt.title(\"Validation Curve\")\n",
    "def to_percent(temp, position):\n",
    "    return '%1.0f'%(100*temp) + '%'\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.xlabel(\"Epchos\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
